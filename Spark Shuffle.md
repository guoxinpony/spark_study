### Spark Shuffle 

##### 1. 未优化的 HashShuffle

**过程：**假设 job 中有 M 个 Mapper 和 N 个 Reducer

1. 每个 Mapper 会在本地磁盘上为每个分区（即每个 Reducer）创建一个对应的 Buffer（缓冲区），并最终溢写（Spill）成独立的文件。因此，一个 Mapper 会产生 N 个临时文件。
2. 当所有 M 个 Mapper 运行结束，Shuffle 阶段产生的磁盘临时文件总数即为 **M * N**

**为什么需要优化：**

1. **IO 压力巨大：** 大量的文件意味着大量的磁盘随机读写（Random I/O），这比顺序读写要慢得多
2. **内存消耗高：** 为了维持这些并发写入的文件，系统需要开启大量的写缓存，极易导致内存溢出（OOM）
3. **网络传输瓶颈：** Reducer 在拉取（Pull）数据时，需要与海量的文件建立连接，严重拖慢了 Shuffle 的效率。



##### 2. 优化后的 HashShuffle：引入**文件合并机制 (File Consolidation)**

**如何优化**：

1. **复用文件句柄：** 属于同一个 CPU 核心（Core）的多个 Task 不再各自创建文件，而是**共用**同一组输出文件。
2. **顺序写入：** 当 Executor 上的第一个 Task 执行时，它会创建 N 个文件；当该核心上的第二个 Task 开始执行时，它不会创建新文件，而是将数据**追加（Append）**到第一个 Task 已经创建好的文件中。

 **解决了以下问题：**

1. **减少文件数量：**未优化Hashshuffle产生**M * N**个文件， 优化后产生**C * N** 个文件，C 为集群的总 CPU 核心数
2. **降低磁盘随机 I/O：** 由于文件总数大幅减少，减少了随机写入，增加了顺序写入，写入性能得到提升
3. **减轻操作系统压力：** 减少了需要同时打开的文件数量，避免了因文件过多导致的内存溢出（OOM）

**局限性：**

1. **输出文件数量依然与 N 正相关：** 如果下游的 Reducer 数量（N）非常大（例如 10,000 个），即使每个核心只创建 N个文件，全集群的文件数依然会非常恐怖。
2. **不支持排序：** HashShuffle 本质上不进行排序。对于需要有序数据的算子（如 `sortByKey`），它在效率上远不如后来成为默认机制的 **SortShuffle**。



##### 3. **SortShuffle**：先排序，后合并

**过程：**

1. **数据写入缓存 (Buffer)：** Task 产生的数据首先会被写入内存数据结构
2. **排序与溢写 (Sort & Spill)：** 当内存达到阈值时，Spark 会根据 **分区 ID (Partition ID)** 对内存中的数据进行排序。排序后的数据被批量溢写到磁盘的一个临时文件（Temp File）中。
3. **合并文件 (Merge)：** 一个 Task 可能会产生多个临时文件。在 Task 结束前，它会将这些临时文件合并成**一个大的磁盘文件 (Data File)**
4. **生成索引文件 (Index File)：** 为了让下游的 Reducer 能在同一个大文件中找到属于自己的那部分数据，Spark 会同步生成一个索引文件。索引文件记录了每个分区在大文件中的 **起始偏移量 (Offset)** 和 **长度**。

**SortShuffle 解决了以下问题：**

1. **文件数量大幅减少：**未优化Hashshuffle产生**M * N**个文件， 优化后的 Hashshuffle 产生**C * N** 个文件；SortShuffle  每个 Map Task 只产生 **1个数据文件 + 1个索引文件**，文件总数为 **2 * M**
2. **极高的扩展性：** 即使下游有 10,000 个 Reducer，SortShuffle 也不会导致文件句柄溢出，非常适合处理大规模数据集。
3.  **IO 性能优化：**  写操作：变为顺序写入缓存；读操作：Reducer 通过索引文件直接定位（Seek）并顺序读取连续的数据块，效率更高。



##### 4. bypassShuffle：节省排序的时间成本

**过程：**

1. **不排序溢写：** 数据在溢写到磁盘时，不进行任何内存排序
2. **分区写入：** 每一个 Task 会根据 Reducer 的数量，直接为每个分区开启一个临时文件并写入数据
3. **汇总合并：** Task 完成后，会将所有的临时分区文件合并成**一个大的磁盘文件**，并生成一个**索引文件 (Index)**

**BypassShuffle 优化了以下部分**：

1. **性能提升（去除了排序开销）：** BypassShuffle 彻底去除了排序过程，大大节省了 CPU 算力和内存消耗，处理速度更快
2. **文件管理优化：** 虽然它像 HashShuffle 一样产生临时分区文件，但在 Task 结束前会将它们合并。因此，它最终产生的磁盘文件数与 SortShuffle 一致，即 **每个 Mapper 只产生 2 个文件（1个数据 + 1个索引）**。这彻底解决了“中间小文件过多”的问题。
3. **降低 IO 风险：** 最终落盘的文件是有序合并的，这使得下游 Reducer 拉取数据时依然是顺序读取，提高了 IO 吞吐量。

**BypassShuffle 的触发条件：**

1.  **下游分区数 (reduce task 数量) 较少：**默认需要小于或等于 200；如果分区太多，同时开启过多的临时文件会导致内存溢出。
2. **算子不需要 Map 端聚合 (Map-side Combine)：**比如 `reduceByKey` 这种算子通常会在 **Map 端先聚合**，这需要排序，因此不能使用 Bypass。而像 `groupByKey` 这种简单的重分区算子则可以使用。



##### 5. Sortshuffle和bypassShuffle 的区别

| **比较维度** | **标准 SortShuffle (图3)**                                   | **BypassShuffle (图4)**                                      |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **内存处理** | 数据进入内存后，必须按分区 ID 进行**排序**。                 | 数据进入内存后，**不进行排序**，直接根据分区路由。           |
| **磁盘写出** | 所有的分区数据交替写入内存，溢写时保持**有序**写出。         | 为**每一个分区**都开启一个独立的临时文件（DiskBlockObjectWriter）。 |
| **合并方式** | 将多个有序的临时文件进行**归并排序（Merge Sort）**，合并成最终文件。 | 简单地将所有分区的临时文件**直接拼接（Concatenate）**起来。  |



**为什么bypassShuffle比 SortShuffle 快:**

1. **省去了 CPU 算力：** 排序是一个 O(n log n) 的过程，且在大规模数据下非常消耗 CPU。Bypass 模式直接跳过这一步。
2. **减少了内存压力：** 不需要专门开辟内存缓冲区来存放待排序的数据结构。
3. **更高效的合并：** 由于临时文件本身就是按分区切分的，最后合并时只需要进行底层的字节流拷贝（File Channel Transfer），比归并排序快得多。



**bypassShuffle的缺陷：**文件句柄（**File Handle**）开销太大

**风险点：** 如果下游有 N 个 Reducer，那么每一个 Mapper 在工作时都要**同时打开 N 个文件**。如果 N 很大（比如 1000 个），会导致系统打开的文件句柄数超限，或者引发频繁的磁盘寻道 I/O。

