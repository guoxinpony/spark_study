### Spark 优化高级 数据倾斜

##### 1. 数据倾斜原理

- 绝大多数task执行得都非常快，但个别task执行极慢。
- 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是业务代码造成的。
- 某个key对应的数据量特别大的话，就会发生数据倾斜。整个Spark作业的运行进度是由运行时间最长的task决定的。



##### 2. 导致数据倾斜的算子

- distinct
- groupByKey
- reduceByKey
- aggregateByKey
- join
- cogroup
- repartition



##### 3. 数据倾斜的解决方案

**解决方案一：使用Hive ETL预处理数据**：

1. **思路**：在Hive阶段离线调度运行， Hive SQL 内部依然会发生数据倾斜，Hive 将处理完的结果，存成一张**中间表**
1. **实践：**Spark 任务启动后，不再访问倾斜的原始表，而是直接 `select` 这张中间表。对于 Spark 来说，它执行的是简单的 **Map-side 操作**，不需要跨节点大规模拉取数据（Shuffle），因此性能提升可能是 10 倍甚至几十倍。
1. **优点：**实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。
1. **缺点：**没有解决倾斜，只是把倾斜从 Spark 引擎搬到了 Hive 引擎；适用场景有限，需要频繁使用Spark对Hive表执行某个分析操作



##### 解决方案二：过滤少数导致倾斜的key

1. **思路**：对于少数几个数据量特别多的key，如果对作业的执行和计算结果不是特别重要的话，**那么就直接过滤掉那少数几个**key。
2. **实践：**Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。
3. **优点：**实现简单，效果好，完全规避掉了数据倾斜
4. **缺点：**适用场景不多，如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么才适合使用这种方案。



##### 解决方案三：提高shuffle操作的并行度

1. **思路**：在执行**shuffle算子**时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。**增加shuffle task的数量**，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了**shuffle** task以后，每个task就分配到一个key，即每个task就处理10条数据，那么每个task的执行时间都会变短。
2. **实践：**这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜
3. **优点：**实现简单，有效缓解数据倾斜的影响
4. **缺点：**缓解了数据倾斜，没有彻底根除问题，数据倾斜分布仍然存在



##### 解决方案四：两阶段聚合（局部聚合+全局聚合）聚合类shuffle算子

1. **思路**：**局部聚合：**将原本相同的key通过**附加随机前缀**的方式，变成多个不同的key，就可以让原本被一个task处理的**数据分散到多个task**上去做局部聚合，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)，聚合后变成了变成了(1_hello, 2) (2_hello, 2)；**全局聚合：** **去除掉随机前缀**，再次进行**全局聚合**，就可以得到最终的结果，比如(hello,2) (hello,2) -> (hello,4)
2. **实践：**对RDD执行**reduceByKey**等聚合类shuffle算子，Spark SQL中使用**group by**进行分组聚合
3. **优点：**对于**聚合类的shuffle**操作导致的数据倾斜，效果好。通常都可以解决数据倾斜，或者大幅度缓解数据倾斜，将Spark作业的**性能提升数倍**以上。
4. **缺点：**只适用于聚合类的shuffle操作， 不适合join类的shuffle操作



##### 解决方案五：将reduce join转为map join（广播连接 Broadcast Join，大表JOIN小表）

1. **思路**： Spark 首先将**小表**的所有数据**拉取到 Driver 端的内存**中，Driver 将这些数据**封装成 Broadcast 变量**，并**分发到**集群中所有运行该任务的 **Executor（执行器）**节点上，每个 Executor 在处理大表的分区数据时，直接从**内存中读取广播变量**完成连接操作。大表完全不移动，大表的每一行数据只需要在本地查表即可完成连接。
2. **实践：**大表（如事实表）关联小表（如维度表），实际生产中，通常建议小表在 **1-2 GB** 以内，否则会造成严重的内存压力或者OOM
3. **优点：**对**join操作**导致的数据倾斜，效果好，不会发生shuffle，解决了数据倾斜。
4. **缺点：**只适用于**大表JOIN小表**的场景；比较**消耗内存资源**，driver和每个Executor内存中都会驻留一份小表的全量数据；如果我们**广播数据量**偏小，如果10G以上，可能发生OOM



##### 解决方案六：采样倾斜key并分拆join操作（两个大表，一表均匀，一表少数KEY倾斜）

1. **思路**： **加盐+局部扩容**，给“倾斜 RDD”中的每条数据打上 n 以内的随机前缀（如 0 - 9），将原本集中的数据分散到多个 Task 中；从另一张表中滤出对应的这几个 Key，将每条数据**膨胀成 n 条**，并依次打上 0 ~ 9 的固定前缀，确保能匹配上侧 A 的随机前缀。
2. **实践：** **采样**得到频率最高的几个 Key，**拆分**原 RDD为仅含这几个倾斜 Key 的 **“倾斜 RDD”**和包含其余 Key 的**“普通 RDD”**，**加盐** 给“倾斜 RDD”中的每条数据打上 n 以内的随机前缀，**扩容** 从另一张表中滤出对应的这几个 Key，将每条数据膨胀成 n 条，并依次打上 0 ~ n-1 的固定前缀，确保能匹配上侧 A 的随机前缀，**合并**倾斜部分 Join结果与普通部分 Join 结果
3. **优点：**对于JOIN类操作的倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join；只对少数 Key 进行膨胀，节省了内存和网络资源。
4. **缺点：**仅适用于**少数几个 Key 导致倾斜**的场景。如果倾斜的 Key 多达成千上万个，该方案会变得极其复杂且低效。



##### 解决方案七：使用随机前缀和扩容RDD进行join （**大量 Key** 存在倾斜）

1. **思路**： **全量加盐+全量扩容**，给倾斜侧 RDD 的每一条数据都加上随机前缀0, 1, ..., n-1 ，原本会汇聚到同一个 Task 的相同 Key，被物理地切分成了 n 份；另一侧 RDD 的每一条数据必须膨胀 n 倍，并分别打上 0, 1, ..., n-1 的固定前缀
2. **实践：** **采样**确认存在大量倾斜KEY，**加盐** 给“倾斜 RDD”中的每条数据打上 n 以内的随机前缀，**扩容** 另一张表每条数据膨胀成 n 条，并依次打上 0 ~ n-1 的固定前缀，确保能匹配上侧 A 的随机前缀，将处理后的两个 RDD 执行传统的 Join 操作
3. **优点：**对所有类型的 Join 数据倾斜基本都能处理，不受倾斜 Key 数量的限制；大幅缩短长尾任务，显著提升作业执行效率
4. **缺点：**以**空间换时间**，对全量数据进行 n 倍扩容，对内存资源的要求极高；**非根治性**，本质上是“**缓解**”了数据倾斜，**增加并行度**让压力分布更均匀，没有像广播连接那样消除 Shuffle 过程
